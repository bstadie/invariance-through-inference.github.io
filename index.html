<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Invariance Through Inference</title>
    <meta name="description"
          content="We introduce a general approach, called Invariance Through Inference,
                for improving the test-time performance of an agent in deployment environments with unknown perceptual variations.
                Instead of producing invariant visual features through interpolation,
                invariance through inference turns adaptation at deployment-time into an unsupervised learning problem.
                This is achieved in practice by deploying a straightforward algorithm that tries to match the distribution of latent features
                to the agent's prior experience, without relying on paired data.
                Although simple, we show that this idea leads to surprising improvements on a variety of adaptation scenarios without access to deployment-time rewards,
                including changes in camera poses and lighting conditions.
                Results are presented on challenging distractor control suite, a robotics environment with image-based observations.">

    <meta name="keywords" content="Deep Reinforcement Learning, Domain Adaptation, Generalization">
    <meta name="author" content="Takuma Yoneda <takuma@ttic.edu>">
    <meta property="og:title" content="Invariance Through Inference">
    <meta property="og:image" content="https://geyang.github.io/plan2vec/thumbnail.jpg">
    <meta name="twitter:creator" content="@takuma_ynd">
    <meta name="twitter:card" content="summary">
    <meta property="og:description"
          content="We introduce a general approach, called Invariance Through Inference,
                    for improving the test-time performance of an agent in deployment environments with unknown perceptual variations.
                    Instead of producing invariant visual features through interpolation,
                    invariance through inference turns adaptation at deployment-time into an unsupervised learning problem.
                    This is achieved in practice by deploying a straightforward algorithm that tries to match the distribution of latent features
                    to the agent's prior experience, without relying on paired data.
                    Although simple, we show that this idea leads to surprising improvements on a variety of adaptation scenarios without access to deployment-time rewards,
                    including changes in camera poses and lighting conditions.
                    Results are presented on challenging distractor control suite, a robotics environment with image-based observations.">
    <link rel="stylesheet" href="./style.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script type="text/javascript" id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
    </script>

</head>
<body>
<article>
    <section id="frontmatter">
        <h1>Invariance Through Inference</h1>
        <h2 id="authors" style="margin-bottom: 0;">Takuma Yoneda,<sup>1</sup> Ge Yang,<sup>2</sup>
            Matthew Walter,<sup>1</sup> Bradly Stadie<sup>1</sup>
        </h2>
        <h3 style="margin-top: 10px;"><sup>1</sup>TTI-Chicago, <sup>2</sup>MIT CSAIL </h3>
        <h3 id="links">
            <a href="https://github.com/takuma-yoneda/invariance-through-inference">CODE</a
            >|<a href="https://example.com">PAPER</a>
        </h3>
        <h2>Overview</h2>
        <p><b>Inverse Through Inference</b> (ITI) is a self-supervised adaptation method for Reinforcement Learning agents.
            ITI adapts an agent's encoder to a target domain without access to its reward.</p>
        <p>
            <video autoplay muted loop playsinline width="60%" height="auto" class="center-wide">
                <source src="media/demo2.webm" type="video/webm">
                <!-- <source src="media/method.mp4" type="video/mp4"> -->
            </video>
        </p>
        <!-- <iframe class="video" width="100%" height="338px" src="https://www.youtube.com/embed/KtvTt3U5bME?rel=0"
             frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
             allowfullscreen></iframe> -->
    </section>
    <h2 id="abstract">Abstract</h2>
    <p>
        We introduce a general approach, called Invariance Through Inference,
        for improving the test-time performance of an agent in deployment environments with unknown perceptual variations.
        Instead of producing invariant visual features through interpolation,
        invariance through inference turns adaptation at deployment-time into an unsupervised learning problem.
        This is achieved in practice by deploying a straightforward algorithm that tries to match the distribution of latent features
        to the agent's prior experience, without relying on paired data.
        Although simple, we show that this idea leads to surprising improvements on a variety of adaptation scenarios without access to deployment-time rewards,
        including changes in camera poses and lighting conditions.
        Results are presented on challenging distractor control suite, a robotics environment with image-based observations.
    </p>

    <h2 id="motivation">Motivation</h3>
    <p>
        Pixel-based RL agent is known to be brittle against distractions.
        A pretrained agent fails to work in a target distracting environment because there's a large shift in observation space.
        A typical approach to this problem is to apply <b>data augmentation</b>. This corresponds to expanding the support of the training distribution.
        <!-- SVEA, DrQ-v2 and SODA are among those methods. -->
    </p>
    <p>
        This makes the agent more robust against distractions, however, as the target distribution becomes far from training,
        we need more and more augmentations, and it becomes infeasible at some point.
        <img src="./media/motivation2.png" alt="motivation" class="center-narrow">
    </p>

    <p>
        Another approach that does not have this issue is <b>domain adaptation</b> that attempts to adapt the network to the target domain.
        In this work, we propose <b>Invariance Through Inference</b> (ITI) that performs self-supervised domain adaptation.
        Especially, ITI adapts a pretrained observation encoder so that the downstream policy \(\pi(a|z)\) can transfer without modification.

        <!-- <img src="./motivation.png" alt="ITI-motivation" class="center"> -->

        The shift in observation space makes the corresponding latent space largely deviate.
        The task of ITI is to bring it back to align the latent structures, by adapting the encoder.
    </p>

    <h2 id="method">Method</h2>
    <p>
        Given an agent pretrained in a source domain, we first deploy a random policy in both source and target domain to collect transitions, and store them to the buffers.
        For the source buffer, we encode observations and store latent transitions \((z_t, a_t, z_{t+1})\).
        <img src="./media/preprocess1.png" alt="ITI-preprocess" class="center-wide">
    </p>
    <p>
        Then, collect random trajectories in the target domain.
        <img src="./media/preprocess2.png" alt="ITI-preprocess" width="60%" class="center">
    </p>
    <p>
        ITI adapts encoder following the two objectives: <i>distribution matching</i> and <i>dynamics consistency</i>.
        <!-- <img src="./method.png" alt="ITI" class="center"> -->
        <video autoplay muted loop playsinline class="center" poster="media/method.png">
            <source src="media/method.webm" type="video/webm">
            <!-- <source src="media/method.mp4" type="video/mp4"> -->
        </video>
        <small>You can download the static figure <a href="https://example.com">here</a></small>

        Distribution matching is achieved via adversarial training,
        and dynamics consistency consists of pretrained forward and backward dynamics network.

    </p>

    <h2 id="experiments">Experiments</h2>
    <p>
        To experiment our adaptation scheme on various type of target domains, we deployed our agents in DeepMind Control suite.
        Especially, we employed a modified version of DistractCS that provides <i>color</i>, <i>camera</i>, and <i>background distractions</i> to DeepMind Control suite.
        <img src="./media/intensity-example.png" alt="intensity-example" class="center">
    </p>
    <p>
        The plot below shows how much gain ITI can achieve on top of base policies.
        DrQ-v2 and SVEA are extensions of SAC that incorporates data augmentation.
        All of the methods are first trained in source domain (i.e., standard, non-distracting domain), and then directly deployed in taget domain (zero-shot).
        Zero-shot performance are shown as dashed lines below.
        As expected, the performance decreases as the intensity goes up.
        <img src="./media/main-result.png" alt="main-result" class="center-wide">
        On top of these, we applied ITI to adapt the encoders.
    </p>
    <p>
        <img src="./media/main-table.png" alt="main-table" class="center-wide">
    </p>


    <h2>BibTex</h2>
    <!-- add copy button here! -->
    <pre>@inproceedings{invr-thru-inf,
    title={Need to fill this after publishing the paper},
    author={Yang, Ge and Zhang, Amy and Morcos, Ari S. and Pineau, Joelle
            and Abbeel, Pieter and Calandra, Roberto},
    booktitle={Proceedings of The 2nd Annual Conference on Learning for Dynamics and Control},
    series={Proceedings of Machine Learning Research},
    pages={1-12},
    year={2020},
    volume={120},
    note={arXiv:2005.03648}
}</pre>
</article>
</body>
</html>
